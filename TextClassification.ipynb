{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMIquGTRD53wggI2l3OKB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aonekoda/test/blob/master/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ-IR8704qqO"
      },
      "source": [
        "# Text classification with the torchtext library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhQpG--y5vB3"
      },
      "source": [
        "### Access to the raw dataset iterator\n",
        "\n",
        "AG_NEWS dataset iterators는 원시 데이터를 레이블과 텍스트의 튜플로 생성한다.\n",
        "\n",
        "AG_NEWS 데이터셋에는 4 종류의 레이블이 달려 있다.\n",
        "\n",
        "1 : World   \n",
        "2 : Sports   \n",
        "3 : Business   \n",
        "4 : Sci/Tec  \n",
        "\n",
        "각 split의 데이터 건수는 \n",
        "train: 120000, test: 7600 이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lrQwlb54Ov4",
        "outputId": "f586b450-b76f-4a13-9547-59d78836af0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = AG_NEWS(root='data', split='train')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.csv: 29.5MB [00:00, 91.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBHq_KX1_V19",
        "outputId": "20856c81-a063-49a1-bb09-6a24205f45c3"
      },
      "source": [
        "train_iter.num_lines"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiJhXsFD49tU",
        "outputId": "c0f0da73-71bb-4a82-d13a-c8b534858c99"
      },
      "source": [
        "next(train_iter)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAd4tkb95MGc"
      },
      "source": [
        "### Prepare data processing pipelines\n",
        "\n",
        "\n",
        "다음은 tokenizer 및 vocabulary를 사용한 일반적인 자연어 데이터 처리의 예입니다.\n",
        "첫 번째 단계는 원시 학습 데이터 세트로 vocabulary를 구축하는 것이다. `torchtext.vocab.Vocab` 클래스의 옵션을 설정하여 사용자 정의의 vocabulary를 생성할 수 있다. 예를 들어 min_freq는 vocaburary에 등록하기 위한 최소 등장 빈도수를 의미한다. [torchtext.vocab.Vocab](https://pytorch.org/text/stable/vocab.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yVr1HcG5C_D",
        "outputId": "2c543a27-2aeb-4aa9-b6de-2ab55d5a7fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "counter = Counter()\n",
        "for (label, line) in train_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "    \n",
        "vocab = Vocab(counter, min_freq=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.csv: 29.5MB [00:00, 79.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Xnl_Kp6Ef4"
      },
      "source": [
        "token 리스트의 값을 정수로 변환할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-pegJGU5-TT",
        "outputId": "643c1ed8-ce8d-4624-9acd-16db40a47ff7"
      },
      "source": [
        "[vocab[token] for token in ['here', 'is', 'an', 'example']]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[476, 22, 31, 5298]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGubpRsg6Sti"
      },
      "source": [
        "text_pipeline은 vocab에 정의된 조회 테이블을 기반으로 텍스트 문자열을 정수 목록으로 변환한다. label_pipeline은 레이블을 정수로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hxcIs5N6Oxi"
      },
      "source": [
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4Je-KedAFRq",
        "outputId": "1191782d-4ee9-4afe-8503-d0d38b29fa41"
      },
      "source": [
        "text_pipeline('here is the an example')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[476, 22, 3, 31, 5298]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFSzK8uuAFg5",
        "outputId": "c840e50c-083a-4ace-a9b4-d25a4f28ac25"
      },
      "source": [
        "label_pipeline('10')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "713547MFAUDJ"
      },
      "source": [
        "### Generate data batch and iterator\n",
        "\n",
        "`torch.utils.data.DataLoader `을 사용하여 원시 데이터를 데이터셋으로 생성할 수 있다.\n",
        "\n",
        "텍스트 원소의 길이가 다를 수 있으므로, 데이터 배치와 오프셋을 생성하기 위한 사용자 함수 collate_batch()를 사용한다. 이 함수는 `torch.utils.data.DataLoader `의 `collate_fn` 인자로 넘겨준다.\n",
        "\n",
        "`collate_fn` 의 입력은 그 크기가 batch_size인 텐서들의 리스트이며, `collate_fn` 은 이들을 미니배치로 묶는 역할을 한다. \n",
        "\n",
        "함수 collate_batch()는 앞서 선언한 `text_pipeline`과 `label_pipeline`을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIVcZw3hAMb3"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch, max_len=50):\n",
        "    label_list, text_list = [], []\n",
        "\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         text_holder = torch.ones(max_len, dtype=torch.int32) # fixed size tensor of max_len\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         pos = min(max_len, len(processed_text))\n",
        "         text_holder[-pos:] = processed_text[-pos:]\n",
        "         text_list.append(text_holder.unsqueeze(dim=0))\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device)    \n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "terhyPk1Jpro"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "모형은 nn.Embedding과 뉴스 분류를 위한 linear layer로 구성된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj00QpePKmgN"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.LSTM = nn.LSTM(embed_dim, hidden_size=64, batch_first=True)\n",
        "        self.fc = nn.Linear(64, num_class)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, _ = self.LSTM(embedded)\n",
        "        last_output = output[:,-1,:]\n",
        "        return self.fc(last_output) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGs8u3PHPVDM"
      },
      "source": [
        "### Initiate instance\n",
        "\n",
        "임베딩 차원이 64 인 모델을 생성한다. vocaburary 크기는 vocaburary 인스턴스의 길이와 같다. 클래스 수는 레이블 수와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At8Xsk8JPqCW"
      },
      "source": [
        "num_class = 4\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "max_len = 50\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Zsr5DkQIm6"
      },
      "source": [
        "### Define functions to train the model and evaluate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIMvpg79QKni"
      },
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predited_label = model(text)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WczsJFz9WCbX"
      },
      "source": [
        "### Split the dataset and run the model\n",
        "\n",
        "원래 AG_NEWS에는 valid 데이터 세트가 없으므로 훈련 데이터 세트를 0.95 (train) 및 0.05 (valid)의 분할 비율로 train / valid 세트로 분할했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7JGH2OiP9ht"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 3 # epoch\n",
        "LR = 0.001 # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvesogBtXFPh",
        "outputId": "fc80aee2-9f99-41f3-f5fe-63fd8e61b1da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "total_accu = None\n",
        "\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = list(train_iter)\n",
        "test_dataset = list(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.csv: 1.86MB [00:00, 56.8MB/s]                  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah706EuHiabU",
        "outputId": "cc67375d-e207-4928-f0d0-630e40051e34"
      },
      "source": [
        "a,b = next(iter(train_dataloader))\n",
        "print(a,'\\n', b)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 0, 3, 0, 0, 0, 1, 1, 2, 2, 1, 0, 1, 0, 2, 1, 0, 2, 3, 3, 3, 0, 1,\n",
            "        0, 2, 1, 0, 2, 1, 1, 0, 3, 2, 1, 3, 0, 2, 3, 3, 3, 0, 3, 2, 2, 2, 0, 1,\n",
            "        3, 3, 1, 1, 2, 2, 0, 2, 3, 0, 0, 2, 0, 0, 0, 0], device='cuda:0') \n",
            " tensor([[    1,     1,     1,  ...,   246,     4,  7115],\n",
            "        [    1,     1,     1,  ...,    11, 17788,     2],\n",
            "        [    1,     1,     1,  ...,    67,   871,     2],\n",
            "        ...,\n",
            "        [    1,     1,   340,  ...,    11,    57,     2],\n",
            "        [    1,     1,     1,  ...,    68,   780,     2],\n",
            "        [    1,     1,     1,  ...,    27,    61,     2]], device='cuda:0',\n",
            "       dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4J5b-YlezqE",
        "outputId": "c051abbe-a8a5-4365-b93c-634d9849a42d"
      },
      "source": [
        "model(b).size()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eglu1rENetXW",
        "outputId": "41ec0ba7-cb55-4302-b9ed-8e4a1017b012"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    \n",
        "    print('Epoch {:3d} : '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           accu_val))\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1 : valid accuracy    0.873 \n",
            "Epoch   2 : valid accuracy    0.894 \n",
            "Epoch   3 : valid accuracy    0.902 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2QkUhy1qU21"
      },
      "source": [
        "### Evaluate the model with test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv8wG2loqS7d",
        "outputId": "24ffdcfa-c81c-4533-c9f7-ec53616ffbf4"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYkw2fSUqcWt"
      },
      "source": [
        "### Test on a random news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x663YQfCqhOe",
        "outputId": "03f4e6e5-4177-42fb-e419-5300447904a5"
      },
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text_holder = torch.ones(100, dtype=torch.int32) \n",
        "        processed_text = torch.tensor(text_pipeline(text))\n",
        "        pos = min(100, len(processed_text))\n",
        "        text_holder[-pos:] = processed_text[-pos:]\n",
        "        output = model(text_holder.unsqueeze(dim=0))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}